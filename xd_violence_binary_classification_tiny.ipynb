{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "111885f3",
   "metadata": {},
   "source": [
    "# Binary Violence Classification with VideoMAE Tiny Model\n",
    "This notebook demonstrates how to fine-tune a VideoMAE tiny model for binary classification on the Violence-XD dataset. Videos are classified as either \"safe\" (non-violent) or \"unsafe\" (violent), where class A represents safe content and all other classes (B1, B2, B4, B5, B6, G) represent unsafe content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b147eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: pytorchvideo in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (0.1.5)\n",
      "Requirement already satisfied: datasets in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: evaluate in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: fvcore in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from pytorchvideo) (0.1.5.post20221221)\n",
      "Requirement already satisfied: av in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from pytorchvideo) (14.2.0)\n",
      "Requirement already satisfied: parameterized in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from pytorchvideo) (0.9.0)\n",
      "Requirement already satisfied: iopath in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from pytorchvideo) (0.1.10)\n",
      "Requirement already satisfied: networkx in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from pytorchvideo) (3.2.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: colorama in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: yacs>=0.1.6 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from fvcore->pytorchvideo) (0.1.8)\n",
      "Requirement already satisfied: termcolor>=1.1 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from fvcore->pytorchvideo) (3.1.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from fvcore->pytorchvideo) (11.2.1)\n",
      "Requirement already satisfied: tabulate in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from fvcore->pytorchvideo) (0.9.0)\n",
      "Requirement already satisfied: portalocker in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from iopath->pytorchvideo) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from portalocker->iopath->pytorchvideo) (310)\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install transformers pytorchvideo datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5cad64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face token successfully loaded from HUGGINGFACE_TOKEN environment variable.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# Read token from environment variable (more secure)\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "if token:\n",
    "    HfFolder.save_token(token)\n",
    "    print(\"Hugging Face token successfully loaded from HUGGINGFACE_TOKEN environment variable.\")\n",
    "else:\n",
    "    print(\"HUGGINGFACE_TOKEN environment variable not set. If you want to push models to the Hub, please set this variable before starting Jupyter Lab.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a740e1c",
   "metadata": {},
   "source": [
    "## Load Violence XD dataset and create binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cc8a543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total video files: 4227\n"
     ]
    }
   ],
   "source": [
    "# Set the path to the local processed dataset folder\n",
    "dataset_root_path = \"processed_dataset\"\n",
    "all_video_file_paths = []\n",
    "\n",
    "with open(os.path.join(dataset_root_path, \"train.csv\"), \"r\") as f:\n",
    "    train_paths = [line.strip().split()[0] for line in f.readlines()]\n",
    "    all_video_file_paths.extend([os.path.join(dataset_root_path, path) for path in train_paths])\n",
    "with open(os.path.join(dataset_root_path, \"val.csv\"), \"r\") as f:\n",
    "    val_paths = [line.strip().split()[0] for line in f.readlines()]\n",
    "    all_video_file_paths.extend([os.path.join(dataset_root_path, path) for path in val_paths])\n",
    "with open(os.path.join(dataset_root_path, \"test.csv\"), \"r\") as f:\n",
    "    test_paths = [line.strip().split()[0] for line in f.readlines()]\n",
    "    all_video_file_paths.extend([os.path.join(dataset_root_path, path) for path in test_paths])\n",
    "print(f\"Total video files: {len(all_video_file_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f4f1fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary classes: 2 - ['safe', 'unsafe']\n",
      "Total samples: 4227\n",
      "Safe samples: 2046\n",
      "Unsafe samples: 2181\n"
     ]
    }
   ],
   "source": [
    "# Create binary classification mapping\n",
    "# A = safe (0), all others (B1, B2, B4, B5, B6, G) = unsafe (1)\n",
    "def map_to_binary_label(original_label):\n",
    "    \"\"\"Map original granular labels to binary labels\"\"\"\n",
    "    main_category = original_label.split('-')[0]\n",
    "    if main_category == 'A':\n",
    "        return 'safe'\n",
    "    else:\n",
    "        return 'unsafe'\n",
    "\n",
    "# Get labels from CSV files and create binary mapping\n",
    "labels = []\n",
    "binary_labels = []\n",
    "for split in [\"train.csv\", \"val.csv\", \"test.csv\"]:\n",
    "    with open(os.path.join(dataset_root_path, split), \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) > 1:\n",
    "                original_label = parts[1]\n",
    "                binary_label = map_to_binary_label(original_label)\n",
    "                labels.append(original_label)\n",
    "                binary_labels.append(binary_label)\n",
    "\n",
    "# Create binary label mappings\n",
    "class_labels = ['safe', 'unsafe']  # 0: safe, 1: unsafe\n",
    "label2id = {'safe': 0, 'unsafe': 1}\n",
    "id2label = {0: 'safe', 1: 'unsafe'}\n",
    "\n",
    "print(f\"Binary classes: {len(label2id)} - {class_labels}\")\n",
    "print(f\"Total samples: {len(binary_labels)}\")\n",
    "print(f\"Safe samples: {binary_labels.count('safe')}\")\n",
    "print(f\"Unsafe samples: {binary_labels.count('unsafe')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d35bb62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set distribution:\n",
      "Safe: 1632 (48.4%)\n",
      "Unsafe: 1740 (51.6%)\n",
      "Class weights - Safe: 1.033, Unsafe: 0.969\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Calculate class weights for binary classification\n",
    "# Count occurrences in training set\n",
    "train_binary_labels = []\n",
    "with open(os.path.join(dataset_root_path, \"train.csv\"), \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) > 1:\n",
    "            original_label = parts[1]\n",
    "            binary_label = map_to_binary_label(original_label)\n",
    "            train_binary_labels.append(binary_label)\n",
    "\n",
    "safe_count = train_binary_labels.count('safe')\n",
    "unsafe_count = train_binary_labels.count('unsafe')\n",
    "total_train = len(train_binary_labels)\n",
    "\n",
    "print(f\"Training set distribution:\")\n",
    "print(f\"Safe: {safe_count} ({100*safe_count/total_train:.1f}%)\")\n",
    "print(f\"Unsafe: {unsafe_count} ({100*unsafe_count/total_train:.1f}%)\")\n",
    "\n",
    "# Calculate class weights (inverse frequency)\n",
    "safe_weight = total_train / (2 * safe_count)\n",
    "unsafe_weight = total_train / (2 * unsafe_count)\n",
    "\n",
    "class_weights = torch.tensor([safe_weight, unsafe_weight], dtype=torch.float)\n",
    "print(f\"Class weights - Safe: {safe_weight:.3f}, Unsafe: {unsafe_weight:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a32a16",
   "metadata": {},
   "source": [
    "## Define and initialize a tiny VideoMAE model for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c186c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with 7688066 parameters\n"
     ]
    }
   ],
   "source": [
    "from transformers import VideoMAEConfig, VideoMAEForVideoClassification, VideoMAEImageProcessor\n",
    "\n",
    "# Tiny model configuration for binary classification\n",
    "binary_config = VideoMAEConfig(\n",
    "    num_hidden_layers=4,  # Reduced from default\n",
    "    hidden_size=384,\n",
    "    intermediate_size=1536,\n",
    "    num_attention_heads=6,\n",
    "    image_size=224,\n",
    "    num_frames=16,\n",
    "    num_labels=2,  # Binary classification\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    mask_ratio=0.0,\n",
    ")\n",
    "\n",
    "model = VideoMAEForVideoClassification(binary_config)\n",
    "image_processor = VideoMAEImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b4bb2a",
   "metadata": {},
   "source": [
    "## Prepare the datasets for binary classification training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e97655d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\order\\.conda\\envs\\videomae\\lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pytorchvideo.data\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey, Normalize, RandomShortSideScale, RemoveKey, ShortSideScale, UniformTemporalSubsample,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    Compose, Lambda, RandomCrop, RandomHorizontalFlip, Resize,\n",
    ")\n",
    "\n",
    "mean = image_processor.image_mean\n",
    "std = image_processor.image_std\n",
    "if \"shortest_edge\" in image_processor.size:\n",
    "    height = width = image_processor.size[\"shortest_edge\"]\n",
    "else:\n",
    "    height = image_processor.size[\"height\"]\n",
    "    width = image_processor.size[\"width\"]\n",
    "resize_to = (height, width)\n",
    "\n",
    "num_frames_to_sample = model.config.num_frames\n",
    "sample_rate = 4\n",
    "fps = 30\n",
    "clip_duration = num_frames_to_sample * sample_rate / fps\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_transform = Compose([\n",
    "    ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=Compose([\n",
    "            UniformTemporalSubsample(num_frames_to_sample),\n",
    "            Lambda(lambda x: x / 255.0),\n",
    "            Normalize(mean, std),\n",
    "            RandomShortSideScale(min_size=256, max_size=320),\n",
    "            RandomCrop(resize_to),\n",
    "            RandomHorizontalFlip(p=0.5),\n",
    "        ]),\n",
    "    ),\n",
    "])\n",
    "\n",
    "def load_labeled_video_paths_binary(csv_filename, root_dir_for_csv_paths, binary_label_map):\n",
    "    \"\"\"Load video paths with binary labels\"\"\"\n",
    "    labeled_paths = []\n",
    "    csv_path = os.path.join(root_dir_for_csv_paths, csv_filename)\n",
    "    with open(csv_path, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                video_path_in_csv = parts[0]\n",
    "                original_label = parts[1]\n",
    "                binary_label = map_to_binary_label(original_label)\n",
    "                full_video_path = os.path.join(root_dir_for_csv_paths, video_path_in_csv)\n",
    "                \n",
    "                if binary_label in binary_label_map:\n",
    "                    label_id = binary_label_map[binary_label]\n",
    "                    labeled_paths.append((full_video_path, {\"label\": label_id}))\n",
    "                else:\n",
    "                    print(f\"Warning: Binary label '{binary_label}' not in label2id map for video {full_video_path}. Skipping.\")\n",
    "            elif line.strip():\n",
    "                print(f\"Warning: Malformed line in {csv_path}: '{line.strip()}'\")\n",
    "    return labeled_paths\n",
    "\n",
    "labeled_video_paths_train = load_labeled_video_paths_binary(\"train.csv\", dataset_root_path, label2id)\n",
    "train_dataset = pytorchvideo.data.LabeledVideoDataset(\n",
    "    labeled_video_paths=labeled_video_paths_train,\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=train_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42fbdd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = Compose([\n",
    "    ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=Compose([\n",
    "            UniformTemporalSubsample(num_frames_to_sample),\n",
    "            Lambda(lambda x: x / 255.0),\n",
    "            Normalize(mean, std),\n",
    "            Resize(resize_to),\n",
    "        ]),\n",
    "    ),\n",
    "])\n",
    "\n",
    "labeled_video_paths_val = load_labeled_video_paths_binary(\"val.csv\", dataset_root_path, label2id)\n",
    "val_dataset = pytorchvideo.data.LabeledVideoDataset(\n",
    "    labeled_video_paths=labeled_video_paths_val,\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    ")\n",
    "\n",
    "labeled_video_paths_test = load_labeled_video_paths_binary(\"test.csv\", dataset_root_path, label2id)\n",
    "test_dataset = pytorchvideo.data.LabeledVideoDataset(\n",
    "    labeled_video_paths=labeled_video_paths_test,\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34828782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes - Train: 3372, Val: 430, Test: 425\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset sizes - Train: {train_dataset.num_videos}, Val: {val_dataset.num_videos}, Test: {test_dataset.num_videos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a982b854",
   "metadata": {},
   "source": [
    "## Training and evaluation setup for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee14015c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\order\\AppData\\Local\\Temp\\ipykernel_31276\\1738086372.py:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedBinaryTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Currently logged in as: mite_gvg (mitegvg) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\BIRKBECK\\REPOS\\videomae-base-finetuned-xd-violence\\wandb\\run-20250726_094449-bwaubybu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mitegvg/huggingface/runs/bwaubybu' target=\"_blank\">videomae-tiny-binary-finetuned-xd-violence</a></strong> to <a href='https://wandb.ai/mitegvg/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mitegvg/huggingface' target=\"_blank\">https://wandb.ai/mitegvg/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mitegvg/huggingface/runs/bwaubybu' target=\"_blank\">https://wandb.ai/mitegvg/huggingface/runs/bwaubybu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1684' max='1684' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1684/1684 4:21:35, Epoch 3/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>True Positives</th>\n",
       "      <th>True Negatives</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.698000</td>\n",
       "      <td>0.643976</td>\n",
       "      <td>0.632362</td>\n",
       "      <td>0.375784</td>\n",
       "      <td>0.515391</td>\n",
       "      <td>0.295689</td>\n",
       "      <td>0.833718</td>\n",
       "      <td>3985</td>\n",
       "      <td>18787</td>\n",
       "      <td>3747</td>\n",
       "      <td>9492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.613200</td>\n",
       "      <td>0.648088</td>\n",
       "      <td>0.639444</td>\n",
       "      <td>0.575853</td>\n",
       "      <td>0.514386</td>\n",
       "      <td>0.654003</td>\n",
       "      <td>0.630736</td>\n",
       "      <td>8814</td>\n",
       "      <td>14213</td>\n",
       "      <td>8321</td>\n",
       "      <td>4663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.636400</td>\n",
       "      <td>0.616777</td>\n",
       "      <td>0.661714</td>\n",
       "      <td>0.535570</td>\n",
       "      <td>0.550772</td>\n",
       "      <td>0.521184</td>\n",
       "      <td>0.745762</td>\n",
       "      <td>7024</td>\n",
       "      <td>16805</td>\n",
       "      <td>5729</td>\n",
       "      <td>6453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.521900</td>\n",
       "      <td>0.605744</td>\n",
       "      <td>0.681514</td>\n",
       "      <td>0.519704</td>\n",
       "      <td>0.596520</td>\n",
       "      <td>0.460414</td>\n",
       "      <td>0.813748</td>\n",
       "      <td>6205</td>\n",
       "      <td>18337</td>\n",
       "      <td>4197</td>\n",
       "      <td>7272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\order\\.conda\\envs\\videomae\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "C:\\Users\\order\\.conda\\envs\\videomae\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "C:\\Users\\order\\.conda\\envs\\videomae\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "C:\\Users\\order\\.conda\\envs\\videomae\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "model_name = \"videomae-tiny-binary\"\n",
    "new_model_name = f\"{model_name}-finetuned-xd-violence\"\n",
    "num_epochs = 4\n",
    "\n",
    "args = TrainingArguments(\n",
    "    new_model_name,\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=True,\n",
    "    max_steps=(train_dataset.num_videos // batch_size) * num_epochs,\n",
    ")\n",
    "\n",
    "# Enhanced metrics for binary classification\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    labels = eval_pred.label_ids\n",
    "    \n",
    "    # Calculate various metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    accuracy = np.mean(predictions == labels)\n",
    "    \n",
    "    # Confusion matrix for additional insights\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, predictions).ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'specificity': specificity,\n",
    "        'true_positives': tp,\n",
    "        'true_negatives': tn,\n",
    "        'false_positives': fp,\n",
    "        'false_negatives': fn\n",
    "    }\n",
    "\n",
    "def collate_fn(examples):\n",
    "    # Permute to (num_frames, num_channels, height, width)\n",
    "    pixel_values = torch.stack([\n",
    "        example[\"video\"].permute(1, 0, 2, 3) for example in examples\n",
    "    ])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "# Custom Trainer with weighted loss for binary classification\n",
    "class WeightedBinaryTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Move class_weights to the device of the logits\n",
    "        weights_on_device = self.class_weights.to(logits.device)\n",
    "        \n",
    "        # Define the loss function with weights\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=weights_on_device)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Instantiate the custom trainer\n",
    "trainer = WeightedBinaryTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    "    class_weights=class_weights\n",
    ")\n",
    "\n",
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7df79d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6f7575db2542fb8ada16fb86ebcd7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1753519487.DESKTOP-JCNIME4.31276.0:   0%|          | 0.00/44.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mitegvg/videomae-tiny-binary-finetuned-xd-violence/commit/73a728eb672c2918630c00c87119fb7ed5462075', commit_message='End of training', commit_description='', oid='73a728eb672c2918630c00c87119fb7ed5462075', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mitegvg/videomae-tiny-binary-finetuned-xd-violence', endpoint='https://huggingface.co', repo_type='model', repo_id='mitegvg/videomae-tiny-binary-finetuned-xd-violence'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54945b3a",
   "metadata": {},
   "source": [
    "## Binary Classification Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0ed6410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use cuda:0\n",
      "C:\\Users\\order\\.conda\\envs\\videomae\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_video\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)}\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 13\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     14\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     15\u001b[0m     probabilities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:1078\u001b[0m, in \u001b[0;36mVideoMAEForVideoClassification.forward\u001b[1;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;124;03m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;124;03meating spaghetti\u001b[39;00m\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1078\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideomae\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1086\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:687\u001b[0m, in \u001b[0;36mVideoMAEModel.forward\u001b[1;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m    684\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m    685\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 687\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbool_masked_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    689\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m    690\u001b[0m     embedding_output,\n\u001b[0;32m    691\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    694\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    695\u001b[0m )\n\u001b[0;32m    696\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:131\u001b[0m, in \u001b[0;36mVideoMAEEmbeddings.forward\u001b[1;34m(self, pixel_values, bool_masked_pos)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pixel_values, bool_masked_pos):\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;66;03m# create patch embeddings\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# add position embeddings\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m embeddings \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mtype_as(embeddings)\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m    135\u001b[0m         device\u001b[38;5;241m=\u001b[39membeddings\u001b[38;5;241m.\u001b[39mdevice, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    136\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:196\u001b[0m, in \u001b[0;36mVideoMAEPatchEmbeddings.forward\u001b[1;34m(self, pixel_values)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# permute to (batch_size, num_channels, num_frames, height, width)\u001b[39;00m\n\u001b[0;32m    195\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m--> 196\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprojection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\conv.py:610\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\conv.py:605\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[0;32m    595\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    596\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    604\u001b[0m     )\n\u001b[1;32m--> 605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "local_model_directory = new_model_name\n",
    "absolute_model_path = os.path.abspath(local_model_directory)\n",
    "\n",
    "video_cls = pipeline(task=\"video-classification\", model=absolute_model_path)\n",
    "\n",
    "# Test inference on a sample video\n",
    "test_video = next(iter(test_dataset))[\"video\"]\n",
    "inputs = {\"pixel_values\": test_video.permute(1, 0, 2, 3).unsqueeze(0)}\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "confidence = probabilities[0][predicted_class_idx].item()\n",
    "\n",
    "print(f\"Predicted class: {model.config.id2label[predicted_class_idx]}\")\n",
    "print(f\"Confidence: {confidence:.3f}\")\n",
    "print(f\"Safe probability: {probabilities[0][0].item():.3f}\")\n",
    "print(f\"Unsafe probability: {probabilities[0][1].item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a85da7",
   "metadata": {},
   "source": [
    "## Comprehensive Binary Classification Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b87df659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting binary classification evaluation on the full test set...\n",
      "Attempting to load model from: D:\\BIRKBECK\\REPOS\\videomae-base-finetuned-xd-violence\\videomae-tiny-binary-finetuned-xd-violence\n",
      "Model directory found. Initializing pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline initialized. Using device: cuda:0\n",
      "Loaded 425 samples from processed_dataset\\test.csv\n",
      "\n",
      "Starting inference on 425 test videos...\n",
      "  Processed 10/425 videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 20/425 videos...\n",
      "  Processed 30/425 videos...\n",
      "  Processed 40/425 videos...\n",
      "  Processed 50/425 videos...\n",
      "  Processed 60/425 videos...\n",
      "  Processed 70/425 videos...\n",
      "  Processed 80/425 videos...\n",
      "  Processed 90/425 videos...\n",
      "  Processed 100/425 videos...\n",
      "  Processed 110/425 videos...\n",
      "  Processed 120/425 videos...\n",
      "  Processed 130/425 videos...\n",
      "  Processed 140/425 videos...\n",
      "  Processed 150/425 videos...\n",
      "  Processed 160/425 videos...\n",
      "  Processed 170/425 videos...\n",
      "  Processed 180/425 videos...\n",
      "  Processed 190/425 videos...\n",
      "  Processed 200/425 videos...\n",
      "  Processed 210/425 videos...\n",
      "  Processed 220/425 videos...\n",
      "  Processed 230/425 videos...\n",
      "  Processed 240/425 videos...\n",
      "  Processed 250/425 videos...\n",
      "  Processed 260/425 videos...\n",
      "  Processed 270/425 videos...\n",
      "  Processed 280/425 videos...\n",
      "  Processed 290/425 videos...\n",
      "  Processed 300/425 videos...\n",
      "  Processed 310/425 videos...\n",
      "  Processed 320/425 videos...\n",
      "  Processed 330/425 videos...\n",
      "  Processed 340/425 videos...\n",
      "  Processed 350/425 videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred during processing of processed_dataset\\videos\\video_000844.mp4: [Errno 1094995529] Invalid data found when processing input: 'processed_dataset\\\\videos\\\\video_000844.mp4'; last error log: [mov,mp4,m4a,3gp,3g2,mj2] moov atom not found\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002786.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000831.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002930.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001140.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002879.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_003772.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000314.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000950.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_003573.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002034.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002397.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_003324.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002362.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002744.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000389.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002364.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001098.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001926.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002720.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_003524.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_003738.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001655.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001247.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000712.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000610.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002918.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001800.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000046.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000992.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001966.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_003355.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001011.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001006.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002548.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_003605.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000616.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_003554.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001352.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002818.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002818.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000258.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_003089.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002264.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_003367.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000974.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_003187.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002220.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001018.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_003693.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000081.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002144.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001225.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000135.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002942.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002276.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002561.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001805.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_003107.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000930.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000481.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001426.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001765.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001870.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000227.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000227.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_003911.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002471.mp4. Skipping.\n",
      "\n",
      "==================================================\n",
      "BINARY CLASSIFICATION EVALUATION RESULTS\n",
      "==================================================\n",
      "Total videos processed: 357\n",
      "Overall Accuracy: 57.14%\n",
      "Average Confidence: 60.86%\n",
      "Average inference time per video: 0.116 seconds (8.63 videos/sec)\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       0.54      0.73      0.62       172\n",
      "      unsafe       0.63      0.42      0.50       185\n",
      "\n",
      "    accuracy                           0.57       357\n",
      "   macro avg       0.58      0.58      0.56       357\n",
      "weighted avg       0.59      0.57      0.56       357\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "                Predicted\n",
      "              Safe  Unsafe\n",
      "Actual Safe    126     46\n",
      "       Unsafe  107     78\n",
      "\n",
      "Additional Binary Classification Metrics:\n",
      "Sensitivity (Unsafe Recall): 0.422\n",
      "Specificity (Safe Recall): 0.733\n",
      "Precision (Unsafe): 0.629\n",
      "Precision (Safe): 0.541\n",
      "\n",
      "Test Set Distribution:\n",
      "Safe videos: 172 (48.2%)\n",
      "Unsafe videos: 185 (51.8%)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import time\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "print(\"Starting binary classification evaluation on the full test set...\")\n",
    "\n",
    "# Define paths\n",
    "dataset_root_path = \"processed_dataset\"\n",
    "test_csv_filename = \"test.csv\"\n",
    "test_csv_path = os.path.join(dataset_root_path, test_csv_filename)\n",
    "\n",
    "local_model_directory = \"videomae-tiny-binary-finetuned-xd-violence\"\n",
    "absolute_model_path = os.path.abspath(local_model_directory)\n",
    "\n",
    "# Function to load test data with binary labels\n",
    "def load_binary_test_data_from_csv(csv_file_path, data_root_path):\n",
    "    test_samples = []\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        print(f\"ERROR: Test CSV file not found at {csv_file_path}\")\n",
    "        return test_samples\n",
    "        \n",
    "    with open(csv_file_path, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                relative_video_path = parts[0]\n",
    "                original_label = parts[1]\n",
    "                binary_label = map_to_binary_label(original_label)\n",
    "                \n",
    "                full_video_path = os.path.normpath(os.path.join(data_root_path, relative_video_path))\n",
    "                test_samples.append((full_video_path, binary_label, original_label))\n",
    "            elif line.strip():\n",
    "                print(f\"Warning: Malformed line in {csv_file_path}: '{line.strip()}'\")\n",
    "    \n",
    "    print(f\"Loaded {len(test_samples)} samples from {csv_file_path}\")\n",
    "    return test_samples\n",
    "\n",
    "# Initialize the video classification pipeline\n",
    "video_cls = None\n",
    "print(f\"Attempting to load model from: {absolute_model_path}\")\n",
    "if not os.path.isdir(absolute_model_path):\n",
    "    print(f\"ERROR: Model directory not found at {absolute_model_path}\")\n",
    "else:\n",
    "    print(f\"Model directory found. Initializing pipeline...\")\n",
    "    try:\n",
    "        video_cls = pipeline(\n",
    "            task=\"video-classification\",\n",
    "            model=absolute_model_path,\n",
    "            device=0 if torch.cuda.is_available() else -1\n",
    "        )\n",
    "        print(f\"Pipeline initialized. Using device: {'cuda:0' if torch.cuda.is_available() else 'cpu'}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing pipeline: {e}\")\n",
    "\n",
    "if video_cls:\n",
    "    # Load test data\n",
    "    test_data = load_binary_test_data_from_csv(test_csv_path, dataset_root_path)\n",
    "\n",
    "    if test_data:\n",
    "        true_labels = []\n",
    "        predicted_labels = []\n",
    "        confidences = []\n",
    "        inference_times = []\n",
    "        total_videos_processed = 0\n",
    "\n",
    "        print(f\"\\nStarting inference on {len(test_data)} test videos...\")\n",
    "        for i, (video_path, true_binary_label, original_label) in enumerate(test_data):\n",
    "            if not os.path.exists(video_path):\n",
    "                print(f\"Warning: Video file not found at {video_path}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                raw_results = video_cls(video_path)\n",
    "                end_time = time.time()\n",
    "                inference_times.append(end_time - start_time)\n",
    "                total_videos_processed += 1\n",
    "\n",
    "                if not raw_results:\n",
    "                    print(f\"Warning: No results returned for video {video_path}. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                # Get the top prediction\n",
    "                top_prediction = raw_results[0]\n",
    "                predicted_label = top_prediction['label']\n",
    "                confidence = top_prediction['score']\n",
    "                \n",
    "                true_labels.append(true_binary_label)\n",
    "                predicted_labels.append(predicted_label)\n",
    "                confidences.append(confidence)\n",
    "                \n",
    "                if (i + 1) % 10 == 0 or (i + 1) == len(test_data):\n",
    "                    print(f\"  Processed {i + 1}/{len(test_data)} videos...\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred during processing of {video_path}: {e}\")\n",
    "\n",
    "        # Calculate comprehensive metrics\n",
    "        if total_videos_processed > 0:\n",
    "            # Convert to numpy arrays for easier manipulation\n",
    "            true_labels_array = np.array(true_labels)\n",
    "            predicted_labels_array = np.array(predicted_labels)\n",
    "            confidences_array = np.array(confidences)\n",
    "            \n",
    "            # Calculate basic metrics\n",
    "            accuracy = np.mean(predicted_labels_array == true_labels_array) * 100\n",
    "            avg_inference_time = sum(inference_times) / len(inference_times)\n",
    "            fps = 1.0 / avg_inference_time if avg_inference_time > 0 else float('inf')\n",
    "            avg_confidence = np.mean(confidences_array) * 100\n",
    "            \n",
    "            # Detailed classification report\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"BINARY CLASSIFICATION EVALUATION RESULTS\")\n",
    "            print(\"=\"*50)\n",
    "            print(f\"Total videos processed: {total_videos_processed}\")\n",
    "            print(f\"Overall Accuracy: {accuracy:.2f}%\")\n",
    "            print(f\"Average Confidence: {avg_confidence:.2f}%\")\n",
    "            print(f\"Average inference time per video: {avg_inference_time:.3f} seconds ({fps:.2f} videos/sec)\")\n",
    "            \n",
    "            print(\"\\nDetailed Classification Report:\")\n",
    "            print(classification_report(true_labels, predicted_labels, target_names=['safe', 'unsafe']))\n",
    "            \n",
    "            print(\"\\nConfusion Matrix:\")\n",
    "            cm = confusion_matrix(true_labels, predicted_labels, labels=['safe', 'unsafe'])\n",
    "            print(\"                Predicted\")\n",
    "            print(\"              Safe  Unsafe\")\n",
    "            print(f\"Actual Safe   {cm[0,0]:4d}   {cm[0,1]:4d}\")\n",
    "            print(f\"       Unsafe {cm[1,0]:4d}   {cm[1,1]:4d}\")\n",
    "            \n",
    "            # Calculate additional metrics\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0  # Recall for unsafe class\n",
    "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0  # Recall for safe class\n",
    "            precision_unsafe = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            precision_safe = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "            \n",
    "            print(f\"\\nAdditional Binary Classification Metrics:\")\n",
    "            print(f\"Sensitivity (Unsafe Recall): {sensitivity:.3f}\")\n",
    "            print(f\"Specificity (Safe Recall): {specificity:.3f}\")\n",
    "            print(f\"Precision (Unsafe): {precision_unsafe:.3f}\")\n",
    "            print(f\"Precision (Safe): {precision_safe:.3f}\")\n",
    "            \n",
    "            # Class distribution in test set\n",
    "            safe_count = list(true_labels).count('safe')\n",
    "            unsafe_count = list(true_labels).count('unsafe')\n",
    "            print(f\"\\nTest Set Distribution:\")\n",
    "            print(f\"Safe videos: {safe_count} ({100*safe_count/len(true_labels):.1f}%)\")\n",
    "            print(f\"Unsafe videos: {unsafe_count} ({100*unsafe_count/len(true_labels):.1f}%)\")\n",
    "            \n",
    "        else:\n",
    "            print(\"\\n--- Evaluation Complete ---\")\n",
    "            print(\"No videos were processed successfully.\")\n",
    "    else:\n",
    "        print(\"No test data loaded. Cannot perform evaluation.\")\n",
    "else:\n",
    "    print(\"Video classification pipeline not initialized. Cannot perform evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3992186f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0831a75",
   "metadata": {},
   "source": [
    "## Model Compression: Pruning and Quantization for Binary Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407ffe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "import os\n",
    "\n",
    "# Load the fine-tuned binary model\n",
    "from transformers import VideoMAEForVideoClassification\n",
    "model_dir = \"videomae-tiny-binary-finetuned-xd-violence\"\n",
    "model = VideoMAEForVideoClassification.from_pretrained(model_dir)\n",
    "\n",
    "print(f\"Original model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# PRUNING: Prune 40% of the weights in all Linear layers (more aggressive for binary task)\n",
    "parameters_to_prune = []\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        parameters_to_prune.append((module, 'weight'))\n",
    "\n",
    "if parameters_to_prune:\n",
    "    prune.global_unstructured(\n",
    "        parameters_to_prune,\n",
    "        pruning_method=prune.L1Unstructured,\n",
    "        amount=0.4,  # Prune 40% of weights globally\n",
    "    )\n",
    "    # Remove pruning re-parametrization\n",
    "    for module, _ in parameters_to_prune:\n",
    "        prune.remove(module, 'weight')\n",
    "    print(f\"Pruned {len(parameters_to_prune)} Linear layers (40% of weights removed).\")\n",
    "else:\n",
    "    print(\"No Linear layers found for pruning.\")\n",
    "\n",
    "# QUANTIZATION: Convert to dynamic quantized version\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Save the compressed model\n",
    "quantized_dir = model_dir + \"-compressed\"\n",
    "os.makedirs(quantized_dir, exist_ok=True)\n",
    "torch.save(quantized_model.state_dict(), os.path.join(quantized_dir, \"pytorch_model.bin\"))\n",
    "model.config.save_pretrained(quantized_dir)\n",
    "\n",
    "print(f\"Compressed model saved to {quantized_dir}\")\n",
    "print(\"The compressed model combines pruning (40%) and int8 quantization for maximum efficiency.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3140a01c",
   "metadata": {},
   "source": [
    "## Evaluate Compressed Binary Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4baba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import VideoMAEForVideoClassification, VideoMAEConfig\n",
    "import time\n",
    "\n",
    "# Load and quantize the model\n",
    "quantized_dir = \"videomae-tiny-binary-finetuned-xd-violence-compressed\"\n",
    "config = VideoMAEConfig.from_pretrained(quantized_dir) if os.path.exists(quantized_dir) else VideoMAEConfig.from_pretrained(\"videomae-tiny-binary-finetuned-xd-violence\")\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\"videomae-tiny-binary-finetuned-xd-violence\", config=config)\n",
    "\n",
    "# Apply quantization\n",
    "model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Evaluate compressed model\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "inference_times = []\n",
    "\n",
    "print(\"Evaluating compressed binary model...\")\n",
    "for i, sample in enumerate(test_dataset):\n",
    "    video = sample[\"video\"].to(device)\n",
    "    label = sample[\"label\"]\n",
    "    \n",
    "    # Prepare input\n",
    "    video = video.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=video)\n",
    "        logits = outputs.logits\n",
    "        prediction = logits.argmax(-1).item()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    inference_times.append(end_time - start_time)\n",
    "    \n",
    "    if prediction == label:\n",
    "        correct_predictions += 1\n",
    "    total_samples += 1\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Processed {i + 1}/{test_dataset.num_videos} videos...\")\n",
    "\n",
    "accuracy = (correct_predictions / total_samples) * 100\n",
    "avg_inference_time = sum(inference_times) / len(inference_times)\n",
    "fps = 1.0 / avg_inference_time\n",
    "\n",
    "print(f\"\\nCompressed Binary Model Results:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Average inference time: {avg_inference_time:.4f} seconds\")\n",
    "print(f\"Inference speed: {fps:.2f} videos/sec\")\n",
    "print(f\"Model size reduction: ~75% (pruning + quantization)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68de59a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
