{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "108692df",
   "metadata": {},
   "source": [
    "# Video Classification with VideoMAE Tiny Model\n",
    "This notebook demonstrates how to fine-tune a much smaller VideoMAE model (\"tiny\" configuration) for the Violence-XD dataset. The model is initialized from scratch with a reduced number of layers and parameters, making it more suitable for resource-constrained environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d6e52e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: pytorchvideo in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (0.1.5)\n",
      "Requirement already satisfied: datasets in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: evaluate in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: fvcore in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from pytorchvideo) (0.1.5.post20221221)\n",
      "Requirement already satisfied: av in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from pytorchvideo) (14.2.0)\n",
      "Requirement already satisfied: parameterized in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from pytorchvideo) (0.9.0)\n",
      "Requirement already satisfied: iopath in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from pytorchvideo) (0.1.10)\n",
      "Requirement already satisfied: networkx in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from pytorchvideo) (3.2.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: colorama in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: yacs>=0.1.6 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from fvcore->pytorchvideo) (0.1.8)\n",
      "Requirement already satisfied: termcolor>=1.1 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from fvcore->pytorchvideo) (3.1.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from fvcore->pytorchvideo) (11.2.1)\n",
      "Requirement already satisfied: tabulate in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from fvcore->pytorchvideo) (0.9.0)\n",
      "Requirement already satisfied: portalocker in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from iopath->pytorchvideo) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\order\\.conda\\envs\\videomae\\lib\\site-packages (from portalocker->iopath->pytorchvideo) (310)\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install transformers pytorchvideo datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "336ebc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face token successfully loaded from HUGGINGFACE_TOKEN environment variable.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# Read token from environment variable (more secure)\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "if token:\n",
    "    HfFolder.save_token(token)\n",
    "    print(\"Hugging Face token successfully loaded from HUGGINGFACE_TOKEN environment variable.\")\n",
    "else:\n",
    "    print(\"HUGGINGFACE_TOKEN environment variable not set. If you want to push models to the Hub, please set this variable before starting Jupyter Lab.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e0ee35",
   "metadata": {},
   "source": [
    "## Load Violence XD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26606d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total video files: 4227\n"
     ]
    }
   ],
   "source": [
    "# Set the path to the local processed dataset folder\n",
    "dataset_root_path = \"processed_dataset\"\n",
    "all_video_file_paths = []\n",
    "\n",
    "with open(os.path.join(dataset_root_path, \"train.csv\"), \"r\") as f:\n",
    "    train_paths = [line.strip().split()[0] for line in f.readlines()]\n",
    "    all_video_file_paths.extend([os.path.join(dataset_root_path, path) for path in train_paths])\n",
    "with open(os.path.join(dataset_root_path, \"val.csv\"), \"r\") as f:\n",
    "    val_paths = [line.strip().split()[0] for line in f.readlines()]\n",
    "    all_video_file_paths.extend([os.path.join(dataset_root_path, path) for path in val_paths])\n",
    "with open(os.path.join(dataset_root_path, \"test.csv\"), \"r\") as f:\n",
    "    test_paths = [line.strip().split()[0] for line in f.readlines()]\n",
    "    all_video_file_paths.extend([os.path.join(dataset_root_path, path) for path in test_paths])\n",
    "print(f\"Total video files: {len(all_video_file_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c8c4d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes: 7.\n",
      "Class labels: ['A', 'B1', 'B2', 'B4', 'B5', 'B6', 'G']\n"
     ]
    }
   ],
   "source": [
    "# Get labels from CSV files\n",
    "labels = []\n",
    "for split in [\"train.csv\", \"val.csv\", \"test.csv\"]:\n",
    "    with open(os.path.join(dataset_root_path, split), \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) > 1:\n",
    "                labels.append(parts[1])\n",
    "class_labels = sorted(set(labels))\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "print(f\"Unique classes: {len(label2id)}.\")\n",
    "print(f\"Class labels: {class_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a04062c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated main category weights: {'A': 0.29516806722689076, 'B1': 1.2383400661035622, 'B2': 1.3646297045730473, 'B6': 1.3685064935064934, 'G': 1.5589459084604715, 'B4': 1.621933621933622, 'B5': 12.042857142857143}\n",
      "Shape of granular_class_weights_tensor: torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Main category counts for the training set (as provided by the user)\n",
    "main_category_counts_train = {\n",
    "    'A': 1632,\n",
    "    'B1': 389,\n",
    "    'B2': 353,\n",
    "    'B6': 352,\n",
    "    'G': 309,\n",
    "    'B4': 297,\n",
    "    'B5': 40\n",
    "}\n",
    "\n",
    "# Calculate total number of main category occurrences and number of unique main categories\n",
    "total_main_category_occurrences = sum(main_category_counts_train.values())\n",
    "num_main_categories = len(main_category_counts_train)\n",
    "\n",
    "# Calculate weights for each main category\n",
    "# Formula: total_samples / (num_classes * class_count)\n",
    "# This gives higher weight to less frequent classes.\n",
    "weights_main_cat = {}\n",
    "for category, count in main_category_counts_train.items():\n",
    "    weights_main_cat[category] = total_main_category_occurrences / (num_main_categories * count)\n",
    "print(f\"Calculated main category weights: {weights_main_cat}\")\n",
    "\n",
    "# id2label is available from the previous cell\n",
    "# Create a tensor for class weights, one weight for each granular label\n",
    "# The weight for a granular label will be the weight of its main category\n",
    "num_granular_labels = len(id2label)\n",
    "granular_class_weights_list = [0.0] * num_granular_labels\n",
    "\n",
    "for i in range(num_granular_labels):\n",
    "    granular_label_str = id2label[i]\n",
    "    main_category_of_granular = granular_label_str.split('-')[0]\n",
    "    if main_category_of_granular in weights_main_cat:\n",
    "        granular_class_weights_list[i] = weights_main_cat[main_category_of_granular]\n",
    "    else:\n",
    "        print(f\"Warning: Main category '{main_category_of_granular}' for label '{granular_label_str}' (ID: {i}) not found in provided main category counts. Assigning weight 1.0.\")\n",
    "        granular_class_weights_list[i] = 1.0 \n",
    "\n",
    "# Convert to a PyTorch tensor\n",
    "granular_class_weights_tensor = torch.tensor(granular_class_weights_list, dtype=torch.float)\n",
    "print(f\"Shape of granular_class_weights_tensor: {granular_class_weights_tensor.shape}\")\n",
    "# print(f\"First few granular weights: {granular_class_weights_tensor[:10]}\")\n",
    "\n",
    "# Ensure the tensor has the correct number of elements\n",
    "assert len(granular_class_weights_tensor) == num_granular_labels, \"Mismatch in granular_class_weights_tensor length\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24adaa0f",
   "metadata": {},
   "source": [
    "## Define and initialize a tiny VideoMAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfff81b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VideoMAEConfig, VideoMAEForVideoClassification, VideoMAEImageProcessor\n",
    "\n",
    "compact_config = VideoMAEConfig(\n",
    "    num_hidden_layers=4,  # Reduced from 6 to 4\n",
    "    hidden_size=384,\n",
    "    intermediate_size=1536,\n",
    "    num_attention_heads=6,\n",
    "    image_size=224,\n",
    "    num_frames=16,\n",
    "    num_labels=len(label2id),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    mask_ratio=0.0,\n",
    ")\n",
    "\n",
    "model = VideoMAEForVideoClassification(compact_config)\n",
    "image_processor = VideoMAEImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df520ad4",
   "metadata": {},
   "source": [
    "## Prepare the datasets for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ccfa68d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorchvideo.data\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey, Normalize, RandomShortSideScale, RemoveKey, ShortSideScale, UniformTemporalSubsample,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    Compose, Lambda, RandomCrop, RandomHorizontalFlip, Resize,\n",
    ")\n",
    "\n",
    "mean = image_processor.image_mean\n",
    "std = image_processor.image_std\n",
    "if \"shortest_edge\" in image_processor.size:\n",
    "    height = width = image_processor.size[\"shortest_edge\"]\n",
    "else:\n",
    "    height = image_processor.size[\"height\"]\n",
    "    width = image_processor.size[\"width\"]\n",
    "resize_to = (height, width)\n",
    "\n",
    "num_frames_to_sample = model.config.num_frames\n",
    "sample_rate = 4\n",
    "fps = 30\n",
    "clip_duration = num_frames_to_sample * sample_rate / fps\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_transform = Compose([\n",
    "    ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=Compose([\n",
    "            UniformTemporalSubsample(num_frames_to_sample),\n",
    "            Lambda(lambda x: x / 255.0),\n",
    "            Normalize(mean, std),\n",
    "            RandomShortSideScale(min_size=256, max_size=320),\n",
    "            RandomCrop(resize_to),\n",
    "            RandomHorizontalFlip(p=0.5),\n",
    "        ]),\n",
    "    ),\n",
    "])\n",
    "\n",
    "def load_labeled_video_paths(csv_filename, root_dir_for_csv_paths, label_to_id_map):\n",
    "    labeled_paths = []\n",
    "    csv_path = os.path.join(root_dir_for_csv_paths, csv_filename)\n",
    "    with open(csv_path, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                video_path_in_csv = parts[0]\n",
    "                label_str = parts[1]\n",
    "                full_video_path = os.path.join(root_dir_for_csv_paths, video_path_in_csv)\n",
    "                if label_str in label_to_id_map:\n",
    "                    label_id = label_to_id_map[label_str]\n",
    "                    labeled_paths.append((full_video_path, {\"label\": label_id}))\n",
    "                else:\n",
    "                    print(f\"Warning: Label '{label_str}' not in label2id map for video {full_video_path}. Skipping.\")\n",
    "            elif line.strip():\n",
    "                print(f\"Warning: Malformed line in {csv_path}: '{line.strip()}'\")\n",
    "    return labeled_paths\n",
    "\n",
    "labeled_video_paths_train = load_labeled_video_paths(\"train.csv\", dataset_root_path, label2id)\n",
    "train_dataset = pytorchvideo.data.LabeledVideoDataset(\n",
    "    labeled_video_paths=labeled_video_paths_train,\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=train_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bbdebee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = Compose([\n",
    "    ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=Compose([\n",
    "            UniformTemporalSubsample(num_frames_to_sample),\n",
    "            Lambda(lambda x: x / 255.0),\n",
    "            Normalize(mean, std),\n",
    "            Resize(resize_to),\n",
    "        ]),\n",
    "    ),\n",
    "])\n",
    "\n",
    "labeled_video_paths_val = load_labeled_video_paths(\"val.csv\", dataset_root_path, label2id)\n",
    "val_dataset = pytorchvideo.data.LabeledVideoDataset(\n",
    "    labeled_video_paths=labeled_video_paths_val,\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    ")\n",
    "\n",
    "labeled_video_paths_test = load_labeled_video_paths(\"test.csv\", dataset_root_path, label2id)\n",
    "test_dataset = pytorchvideo.data.LabeledVideoDataset(\n",
    "    labeled_video_paths=labeled_video_paths_test,\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "016a4b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3372 430 425\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.num_videos, val_dataset.num_videos, test_dataset.num_videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b79ecc4",
   "metadata": {},
   "source": [
    "## Training and evaluation setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe2ea7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\order\\AppData\\Local\\Temp\\ipykernel_24064\\4043090594.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch # Ensure torch is imported for nn.CrossEntropyLoss\n",
    "\n",
    "model_name = \"videomae-tiny\"\n",
    "new_model_name = f\"{model_name}-finetuned-xd-violence\"\n",
    "num_epochs = 4\n",
    "\n",
    "args = TrainingArguments(\n",
    "    new_model_name,\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True,\n",
    "    max_steps=(train_dataset.num_videos // batch_size) * num_epochs,\n",
    ")\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "def collate_fn(examples):\n",
    "    # Permute to (num_frames, num_channels, height, width)\n",
    "    pixel_values = torch.stack([\n",
    "        example[\"video\"].permute(1, 0, 2, 3) for example in examples\n",
    "    ])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "# Define the custom Trainer with weighted loss\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # class_weights should be a tensor\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs): # MODIFIED SIGNATURE\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Move class_weights to the device of the logits\n",
    "        weights_on_device = self.class_weights.to(logits.device)\n",
    "        \n",
    "        # Define the loss function with weights\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=weights_on_device)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Instantiate the custom trainer\n",
    "# granular_class_weights_tensor is computed in the cell inserted before this block\n",
    "trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=image_processor, # Though not strictly a tokenizer, it's used for processor/config by Trainer\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    "    class_weights=granular_class_weights_tensor # Pass the calculated weights here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e38f8060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\order\\AppData\\Local\\Temp\\ipykernel_24064\\671122403.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1684' max='1684' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1684/1684 4:19:02, Epoch 3/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.934700</td>\n",
       "      <td>1.521377</td>\n",
       "      <td>0.560329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.707500</td>\n",
       "      <td>1.616424</td>\n",
       "      <td>0.324401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.702500</td>\n",
       "      <td>1.700620</td>\n",
       "      <td>0.311433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.804400</td>\n",
       "      <td>1.521430</td>\n",
       "      <td>0.414484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\order\\.conda\\envs\\videomae\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "C:\\Users\\order\\.conda\\envs\\videomae\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "C:\\Users\\order\\.conda\\envs\\videomae\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "C:\\Users\\order\\.conda\\envs\\videomae\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch # Ensure torch is imported for nn.CrossEntropyLoss\n",
    "\n",
    "model_name = \"videomae-tiny\"\n",
    "new_model_name = f\"{model_name}-finetuned-xd-violence\"\n",
    "num_epochs = 4\n",
    "\n",
    "args = TrainingArguments(\n",
    "    new_model_name,\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True,\n",
    "    max_steps=(train_dataset.num_videos // batch_size) * num_epochs,\n",
    ")\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "def collate_fn(examples):\n",
    "    # Permute to (num_frames, num_channels, height, width)\n",
    "    pixel_values = torch.stack([\n",
    "        example[\"video\"].permute(1, 0, 2, 3) for example in examples\n",
    "    ])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "# Define the custom Trainer with weighted loss\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # class_weights should be a tensor\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs): # MODIFIED SIGNATURE to include **kwargs\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Move class_weights to the device of the logits\n",
    "        weights_on_device = self.class_weights.to(logits.device)\n",
    "        \n",
    "        # Define the loss function with weights\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=weights_on_device)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Instantiate the custom trainer\n",
    "# granular_class_weights_tensor is computed in the cell inserted before this block\n",
    "trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=image_processor, # Though not strictly a tokenizer, it's used for processor/config by Trainer\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    "    class_weights=granular_class_weights_tensor # Pass the calculated weights here\n",
    ")\n",
    "\n",
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9420dc55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9545ab6dc0d143e1a51feeaa475e84d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1749831415.DESKTOP-JCNIME4:   0%|          | 0.00/41.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mitegvg/videomae-tiny-finetuned-xd-violence/commit/30f8443baa75990db8d6bfd7a34d573ae956141d', commit_message='End of training', commit_description='', oid='30f8443baa75990db8d6bfd7a34d573ae956141d', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mitegvg/videomae-tiny-finetuned-xd-violence', endpoint='https://huggingface.co', repo_type='model', repo_id='mitegvg/videomae-tiny-finetuned-xd-violence'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9aa12a",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e4d3b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "C:\\Users\\order\\.conda\\envs\\videomae\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_video\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)}\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 12\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     13\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     14\u001b[0m predicted_class_idx \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:1078\u001b[0m, in \u001b[0;36mVideoMAEForVideoClassification.forward\u001b[1;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;124;03m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;124;03meating spaghetti\u001b[39;00m\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1078\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideomae\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1086\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:687\u001b[0m, in \u001b[0;36mVideoMAEModel.forward\u001b[1;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m    684\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m    685\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 687\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbool_masked_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    689\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m    690\u001b[0m     embedding_output,\n\u001b[0;32m    691\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    694\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    695\u001b[0m )\n\u001b[0;32m    696\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:131\u001b[0m, in \u001b[0;36mVideoMAEEmbeddings.forward\u001b[1;34m(self, pixel_values, bool_masked_pos)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pixel_values, bool_masked_pos):\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;66;03m# create patch embeddings\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# add position embeddings\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m embeddings \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mtype_as(embeddings)\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m    135\u001b[0m         device\u001b[38;5;241m=\u001b[39membeddings\u001b[38;5;241m.\u001b[39mdevice, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    136\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:196\u001b[0m, in \u001b[0;36mVideoMAEPatchEmbeddings.forward\u001b[1;34m(self, pixel_values)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# permute to (batch_size, num_channels, num_frames, height, width)\u001b[39;00m\n\u001b[0;32m    195\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m--> 196\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprojection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\conv.py:610\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\conv.py:605\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[0;32m    595\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    596\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    604\u001b[0m     )\n\u001b[1;32m--> 605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "local_model_directory = new_model_name\n",
    "absolute_model_path = os.path.abspath(local_model_directory)\n",
    "\n",
    "video_cls = pipeline(task=\"video-classification\", model=absolute_model_path)\n",
    "\n",
    "test_video = next(iter(test_dataset))[\"video\"]\n",
    "inputs = {\"pixel_values\": test_video.permute(1, 0, 2, 3).unsqueeze(0)}\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c32d2bc-37d4-4dd1-a6b6-1e1ed9912738",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dc7cbb0-12b9-4a08-bc70-9f97c99c6577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation on the full test set...\n",
      "Attempting to load model from: D:\\BIRKBECK\\REPOS\\videomae-base-finetuned-xd-violence\\videomae-tiny-finetuned-xd-violence\n",
      "Model directory found. Initializing pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline initialized. Using device: cuda:0\n",
      "Loaded 425 samples from processed_dataset\\test.csv\n",
      "\n",
      "Starting inference on 425 test videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 10/425 videos...\n",
      "  Processed 20/425 videos...\n",
      "  Processed 30/425 videos...\n",
      "  Processed 40/425 videos...\n",
      "  Processed 50/425 videos...\n",
      "  Processed 60/425 videos...\n",
      "  Processed 70/425 videos...\n",
      "  Processed 80/425 videos...\n",
      "  Processed 90/425 videos...\n",
      "  Processed 100/425 videos...\n",
      "  Processed 110/425 videos...\n",
      "  Processed 120/425 videos...\n",
      "  Processed 130/425 videos...\n",
      "  Processed 140/425 videos...\n",
      "  Processed 150/425 videos...\n",
      "  Processed 160/425 videos...\n",
      "  Processed 170/425 videos...\n",
      "  Processed 180/425 videos...\n",
      "  Processed 190/425 videos...\n",
      "  Processed 200/425 videos...\n",
      "  Processed 210/425 videos...\n",
      "  Processed 220/425 videos...\n",
      "  Processed 230/425 videos...\n",
      "  Processed 240/425 videos...\n",
      "  Processed 250/425 videos...\n",
      "  Processed 260/425 videos...\n",
      "  Processed 270/425 videos...\n",
      "  Processed 280/425 videos...\n",
      "  Processed 290/425 videos...\n",
      "  Processed 300/425 videos...\n",
      "  Processed 310/425 videos...\n",
      "  Processed 320/425 videos...\n",
      "  Processed 330/425 videos...\n",
      "  Processed 340/425 videos...\n",
      "  Processed 350/425 videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred during processing of processed_dataset\\videos\\video_000844.mp4: [Errno 1094995529] Invalid data found when processing input: 'processed_dataset\\\\videos\\\\video_000844.mp4'; last error log: [mov,mp4,m4a,3gp,3g2,mj2] moov atom not found\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002786.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000831.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002930.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001140.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002879.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_003772.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000314.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000950.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_003573.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002034.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002397.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_003324.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002362.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002744.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000389.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002364.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001098.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001926.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002720.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_003524.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_003738.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001655.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001247.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000712.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000610.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002918.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001800.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000046.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000992.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001966.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_003355.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001011.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001006.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002548.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_003605.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000616.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_003554.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001352.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002818.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002818.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000258.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_003089.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002264.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_003367.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000974.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_003187.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002220.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001018.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_003693.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000081.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002144.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001225.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000135.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002942.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002276.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002561.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001805.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_003107.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000930.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000481.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001426.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001765.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_001870.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000227.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_000227.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_003911.mp4. Skipping.\n",
      "Warning: Video file not found at processed_dataset\\videos\\video_002471.mp4. Skipping.\n",
      "\n",
      "--- Evaluation Complete ---\n",
      "Total videos processed: 357\n",
      "Top-1 Correct Predictions: 141\n",
      "Top-5 Correct Predictions: 321\n",
      "Top-1 Accuracy: 39.50%\n",
      "Top-5 Accuracy: 89.92%\n",
      "Average inference time per video: 0.069 seconds (14.54 videos/sec)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import pipeline\n",
    "import torch # For checking device\n",
    "import time # For timing inference\n",
    "\n",
    "print(\"Starting evaluation on the full test set...\")\n",
    "\n",
    "# Define paths (relative to the notebook location)\n",
    "dataset_root_path = \"processed_dataset\"\n",
    "test_csv_filename = \"test.csv\"\n",
    "test_csv_path = os.path.join(dataset_root_path, test_csv_filename)\n",
    "\n",
    "local_model_directory = \"videomae-tiny-finetuned-xd-violence\"\n",
    "absolute_model_path = os.path.abspath(local_model_directory)\n",
    "\n",
    "# Function to load test data (video paths and true labels)\n",
    "def load_test_data_from_csv(csv_file_path, data_root_path):\n",
    "    test_samples = []\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        print(f\"ERROR: Test CSV file not found at {csv_file_path}\")\n",
    "        return test_samples\n",
    "        \n",
    "    with open(csv_file_path, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                relative_video_path = parts[0]  # e.g., \"videos/video_000000.mp4\"\n",
    "                true_label_str = parts[1]       # e.g., \"A\", \"B1\"\n",
    "                \n",
    "                # Construct the full path to the video file\n",
    "                # data_root_path is dataset_root_path (e.g., \"processed_dataset\")\n",
    "                full_video_path = os.path.normpath(os.path.join(data_root_path, relative_video_path))\n",
    "                test_samples.append((full_video_path, true_label_str))\n",
    "            elif line.strip(): # Avoid warning for empty lines if any\n",
    "                print(f\"Warning: Malformed line in {csv_file_path}: '{line.strip()}'\") # Corrected escaping here\n",
    "    print(f\"Loaded {len(test_samples)} samples from {csv_file_path}\")\n",
    "    return test_samples\n",
    "\n",
    "# Initialize the video classification pipeline\n",
    "video_cls = None\n",
    "print(f\"Attempting to load model from: {absolute_model_path}\")\n",
    "if not os.path.isdir(absolute_model_path):\n",
    "    print(f\"ERROR: Model directory not found at {absolute_model_path}\")\n",
    "else:\n",
    "    print(f\"Model directory found. Initializing pipeline...\")\n",
    "    try:\n",
    "        video_cls = pipeline(\n",
    "            task=\"video-classification\",\n",
    "            model=absolute_model_path,\n",
    "            device=0 if torch.cuda.is_available() else -1 # Use GPU if available\n",
    "        )\n",
    "        print(f\"Pipeline initialized. Using device: {'cuda:0' if torch.cuda.is_available() else 'cpu'}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing pipeline: {e}\")\n",
    "\n",
    "if video_cls:\n",
    "    # Load test data\n",
    "    test_data = load_test_data_from_csv(test_csv_path, dataset_root_path)\n",
    "\n",
    "    if test_data:\n",
    "        top1_correct_predictions = 0\n",
    "        top5_correct_predictions = 0\n",
    "        total_videos_processed = 0\n",
    "        inference_times = []  # Store inference times\n",
    "\n",
    "        print(f\"\\nStarting inference on {len(test_data)} test videos...\") # Corrected escaping for newline\n",
    "        for i, (video_path, true_label) in enumerate(test_data):\n",
    "            if not os.path.exists(video_path):\n",
    "                print(f\"Warning: Video file not found at {video_path}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Measure inference time\n",
    "                start_time = time.time()\n",
    "                raw_results = video_cls(video_path)\n",
    "                end_time = time.time()\n",
    "                inference_times.append(end_time - start_time)\n",
    "                total_videos_processed += 1\n",
    "\n",
    "                if not raw_results:\n",
    "                    print(f\"Warning: No results returned for video {video_path}. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                # Extract top 5 predicted labels (main part, e.g., \"B4\" from \"B4-0-0\")\n",
    "                # Corrected line:\n",
    "                predicted_labels_top5 = [res['label'].split('-')[0] for res in raw_results[:5]]\n",
    "\n",
    "                if not predicted_labels_top5:\n",
    "                    print(f\"Warning: Could not extract top 5 labels for {video_path}. Skipping.\")\n",
    "                    continue\n",
    "                    \n",
    "                predicted_label_top1 = predicted_labels_top5[0]\n",
    "\n",
    "                # Check Top-1 accuracy\n",
    "                if predicted_label_top1 == true_label:\n",
    "                    top1_correct_predictions += 1\n",
    "                \n",
    "                # Check Top-5 accuracy\n",
    "                if true_label in predicted_labels_top5:\n",
    "                    top5_correct_predictions += 1\n",
    "                \n",
    "                if (i + 1) % 10 == 0 or (i + 1) == len(test_data): # Print progress\n",
    "                    print(f\"  Processed {i + 1}/{len(test_data)} videos...\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred during processing of {video_path}: {e}\")\n",
    "\n",
    "        # Calculate accuracies\n",
    "        if total_videos_processed > 0:\n",
    "            top1_accuracy = (top1_correct_predictions / total_videos_processed) * 100\n",
    "            top5_accuracy = (top5_correct_predictions / total_videos_processed) * 100\n",
    "            avg_inference_time = sum(inference_times) / len(inference_times)\n",
    "            fps = 1.0 / avg_inference_time if avg_inference_time > 0 else float('inf')\n",
    "            \n",
    "            print(\"\\n--- Evaluation Complete ---\") # Corrected escaping for newline\n",
    "            print(f\"Total videos processed: {total_videos_processed}\")\n",
    "            print(f\"Top-1 Correct Predictions: {top1_correct_predictions}\")\n",
    "            print(f\"Top-5 Correct Predictions: {top5_correct_predictions}\")\n",
    "            print(f\"Top-1 Accuracy: {top1_accuracy:.2f}%\")\n",
    "            print(f\"Top-5 Accuracy: {top5_accuracy:.2f}%\")\n",
    "            print(f\"Average inference time per video: {avg_inference_time:.3f} seconds ({fps:.2f} videos/sec)\")\n",
    "        else:\n",
    "            print(\"\\n--- Evaluation Complete ---\") # Corrected escaping for newline\n",
    "            print(\"No videos were processed successfully.\")\n",
    "    else:\n",
    "        print(\"No test data loaded. Cannot perform evaluation.\")\n",
    "else:\n",
    "    print(\"Video classification pipeline not initialized. Cannot perform evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3aea45-3af9-486a-8779-bba18c9ed1c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f7c9266",
   "metadata": {},
   "source": [
    "## FPS Benchmark: VideoMAE Base (HuggingFace)\n",
    "This cell benchmarks the inference speed (FPS) of the original VideoMAE Base model from HuggingFace on all valid test videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a322abea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempted 10/330 videos...\n",
      "  Attempted 20/330 videos...\n",
      "  Attempted 30/330 videos...\n",
      "  Attempted 40/330 videos...\n",
      "  Attempted 50/330 videos...\n",
      "  Attempted 60/330 videos...\n",
      "  Attempted 70/330 videos...\n",
      "  Attempted 80/330 videos...\n",
      "  Attempted 90/330 videos...\n",
      "  Attempted 100/330 videos...\n",
      "  Attempted 110/330 videos...\n",
      "  Attempted 120/330 videos...\n",
      "  Attempted 130/330 videos...\n",
      "  Attempted 140/330 videos...\n",
      "  Attempted 150/330 videos...\n",
      "  Attempted 160/330 videos...\n",
      "  Attempted 170/330 videos...\n",
      "  Attempted 180/330 videos...\n",
      "  Attempted 190/330 videos...\n",
      "  Attempted 200/330 videos...\n",
      "  Attempted 210/330 videos...\n",
      "  Attempted 220/330 videos...\n",
      "  Attempted 230/330 videos...\n",
      "  Attempted 240/330 videos...\n",
      "  Attempted 250/330 videos...\n",
      "  Attempted 260/330 videos...\n",
      "  Attempted 270/330 videos...\n",
      "  Attempted 280/330 videos...\n",
      "  Attempted 290/330 videos...\n",
      "  Attempted 300/330 videos...\n",
      "  Attempted 310/330 videos...\n",
      "  Attempted 320/330 videos...\n",
      "Skipping processed_dataset\\videos\\video_000844.mp4: PyAV error: [Errno 1094995529] Invalid data found when processing input: 'processed_dataset\\\\videos\\\\video_000844.mp4'; last error log: [mov,mp4,m4a,3gp,3g2,mj2] moov atom not found\n",
      "VideoMAE Base - Average inference time per video: 0.086 seconds (FPS: 11.63) on 329 valid videos.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Use the official VideoMAE Base model from HuggingFace\n",
    "base_model_id = \"MCG-NJU/videomae-base\"\n",
    "\n",
    "# Initialize pipeline for video classification\n",
    "video_cls_base = pipeline(\n",
    "    task=\"video-classification\",\n",
    "    model=base_model_id,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Load all test videos for benchmarking\n",
    "dataset_root_path = \"processed_dataset\"\n",
    "test_csv_path = os.path.join(dataset_root_path, \"test.csv\")\n",
    "\n",
    "# Helper to load all video paths\n",
    "video_paths = []\n",
    "with open(test_csv_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) >= 1:\n",
    "            rel_path = parts[0]\n",
    "            full_path = os.path.normpath(os.path.join(dataset_root_path, rel_path))\n",
    "            if os.path.exists(full_path):\n",
    "                video_paths.append(full_path)\n",
    "\n",
    "# Benchmark FPS on all test videos\n",
    "inference_times = []\n",
    "valid_videos = 0\n",
    "for i, video_path in enumerate(video_paths):\n",
    "    try:\n",
    "        # Try opening with PyAV first to check for corruption before pipeline\n",
    "        import av\n",
    "        try:\n",
    "            container = av.open(video_path)\n",
    "            container.close()\n",
    "        except Exception as av_err:\n",
    "            print(f\"Skipping {video_path}: PyAV error: {av_err}\")\n",
    "            continue\n",
    "        start = time.time()\n",
    "        _ = video_cls_base(video_path)\n",
    "        end = time.time()\n",
    "        inference_times.append(end - start)\n",
    "        valid_videos += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {video_path}: {e}\")\n",
    "    if (i + 1) % 10 == 0 or (i + 1) == len(video_paths):\n",
    "        print(f\"  Attempted {i + 1}/{len(video_paths)} videos...\")\n",
    "\n",
    "if inference_times:\n",
    "    avg_time = sum(inference_times) / len(inference_times)\n",
    "    fps = 1.0 / avg_time if avg_time > 0 else float('inf')\n",
    "    print(f\"VideoMAE Base - Average inference time per video: {avg_time:.3f} seconds (FPS: {fps:.2f}) on {valid_videos} valid videos.\")\n",
    "else:\n",
    "    print(\"No valid test videos found for benchmarking.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a55283-1b38-4937-ad30-09c751e09b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c031d2b",
   "metadata": {},
   "source": [
    "## Prune and Quantize the Fine-tuned Model\n",
    "This cell demonstrates how to prune and quantize the trained VideoMAE model to further reduce its size and improve inference speed. Pruning removes less important weights, and quantization reduces the precision of weights from float32 to int8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65f413a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned 25 Linear layers.\n",
      "Quantized and pruned model weights saved to videomae-tiny-finetuned-xd-violence-pruned-quantized/pytorch_model.bin\n",
      "Model config saved to videomae-tiny-finetuned-xd-violence-pruned-quantized/config.json\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "import os\n",
    "\n",
    "# Load the fine-tuned model\n",
    "from transformers import VideoMAEForVideoClassification\n",
    "model_dir = \"videomae-tiny-finetuned-xd-violence\"\n",
    "model = VideoMAEForVideoClassification.from_pretrained(model_dir)\n",
    "\n",
    "# PRUNING: Prune 30% of the weights in all Linear layers\n",
    "parameters_to_prune = []\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        parameters_to_prune.append((module, 'weight'))\n",
    "if parameters_to_prune:\n",
    "    prune.global_unstructured(\n",
    "        parameters_to_prune,\n",
    "        pruning_method=prune.L1Unstructured,\n",
    "        amount=0.3,  # Prune 30% of weights globally\n",
    "    )\n",
    "    # Remove pruning re-parametrization so the model can be saved normally\n",
    "    for module, _ in parameters_to_prune:\n",
    "        prune.remove(module, 'weight')\n",
    "    print(f\"Pruned {len(parameters_to_prune)} Linear layers.\")\n",
    "else:\n",
    "    print(\"No Linear layers found for pruning.\")\n",
    "\n",
    "# QUANTIZATION: Convert model to dynamic quantized version (int8 for Linear layers)\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Save the quantized and pruned model using torch.save\n",
    "quantized_dir = model_dir + \"-pruned-quantized\"\n",
    "os.makedirs(quantized_dir, exist_ok=True)\n",
    "torch.save(quantized_model.state_dict(), os.path.join(quantized_dir, \"pytorch_model.bin\"))\n",
    "model.config.save_pretrained(quantized_dir)\n",
    "print(f\"Quantized and pruned model weights saved to {quantized_dir}/pytorch_model.bin\")\n",
    "print(f\"Model config saved to {quantized_dir}/config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0895b49a",
   "metadata": {},
   "source": [
    "You can now use the pruned and quantized model for inference as usual, or compare its file size and speed to the original."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8046957",
   "metadata": {},
   "source": [
    "## Evaluate the Pruned and Quantized Model\n",
    "This cell loads the pruned and quantized model and evaluates its top-1 and top-5 accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e86d78b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'weight' must be Tensor, not method",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m video \u001b[38;5;241m=\u001b[39m video\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 28\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     30\u001b[0m     top5 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(logits, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mindices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:1078\u001b[0m, in \u001b[0;36mVideoMAEForVideoClassification.forward\u001b[1;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;124;03m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;124;03meating spaghetti\u001b[39;00m\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1078\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideomae\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1086\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:689\u001b[0m, in \u001b[0;36mVideoMAEModel.forward\u001b[1;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    685\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    687\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values, bool_masked_pos)\n\u001b[1;32m--> 689\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    696\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:472\u001b[0m, in \u001b[0;36mVideoMAEEncoder.forward\u001b[1;34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    465\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    466\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    467\u001b[0m         hidden_states,\n\u001b[0;32m    468\u001b[0m         layer_head_mask,\n\u001b[0;32m    469\u001b[0m         output_attentions,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 472\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:416\u001b[0m, in \u001b[0;36mVideoMAELayer.forward\u001b[1;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    412\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    413\u001b[0m     head_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    414\u001b[0m     output_attentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    415\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m--> 416\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayernorm_before\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# in VideoMAE, layernorm is applied before self-attention\u001b[39;49;00m\n\u001b[0;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    421\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    422\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:355\u001b[0m, in \u001b[0;36mVideoMAEAttention.forward\u001b[1;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    351\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    352\u001b[0m     head_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    353\u001b[0m     output_attentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m--> 355\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    357\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    359\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:267\u001b[0m, in \u001b[0;36mVideoMAESelfAttention.forward\u001b[1;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28mself\u001b[39m, hidden_states, head_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, output_attentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    265\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[0;32m    266\u001b[0m     k_bias \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_bias, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m     values \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mhidden_states, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_bias)\n\u001b[0;32m    269\u001b[0m     queries \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mhidden_states, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery\u001b[38;5;241m.\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_bias)\n",
      "\u001b[1;31mTypeError\u001b[0m: linear(): argument 'weight' must be Tensor, not method"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import VideoMAEForVideoClassification, VideoMAEConfig\n",
    "import os\n",
    "\n",
    "# Load config and original (float) model\n",
    "quantized_dir = \"videomae-tiny-finetuned-xd-violence-pruned-quantized\"\n",
    "config = VideoMAEConfig.from_pretrained(quantized_dir)\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\"videomae-tiny-finetuned-xd-violence\", config=config)\n",
    "\n",
    "# Quantize in memory\n",
    "model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Evaluate on test set\n",
    "top1_correct = 0\n",
    "top5_correct = 0\n",
    "total = 0\n",
    "\n",
    "for sample in test_dataset:\n",
    "    video = sample[\"video\"].to(device)  # (C, T, H, W)\n",
    "    label = sample[\"label\"]\n",
    "    # Permute and unsqueeze to (1, T, C, H, W)\n",
    "    video = video.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=video)\n",
    "        logits = outputs.logits\n",
    "        top5 = torch.topk(logits, k=5, dim=-1).indices[0].cpu().numpy()\n",
    "        pred = logits.argmax(-1).item()\n",
    "        if pred == label:\n",
    "            top1_correct += 1\n",
    "        if label in top5:\n",
    "            top5_correct += 1\n",
    "        total += 1\n",
    "\n",
    "print(f\"Pruned+Quantized Model - Top-1 Accuracy: {100*top1_correct/total:.2f}%\")\n",
    "print(f\"Pruned+Quantized Model - Top-5 Accuracy: {100*top5_correct/total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fce759-5849-4a87-9400-b701050be9bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
